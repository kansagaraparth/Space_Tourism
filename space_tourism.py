# -*- coding: utf-8 -*-
"""Space Tourism.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nDE74d5IyCMon-_82rQVjqS_4HUsjeAl
"""

!pip install langchain langchain-community langchain-google-genai faiss-cpu sentence-transformers pymupdf

from google.colab import files
uploaded = files.upload()

from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain_google_genai import ChatGoogleGenerativeAI
import os

os.environ["GOOGLE_API_KEY"] = "AIzaSyBXe6-zagB7mf7z6VJVBgiUh-HQ7ftfngI"

loader = TextLoader("space_tourism_future.txt")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunked_docs = splitter.split_documents(documents)

print(f"Loaded {len(documents)} document(s), split into {len(chunked_docs)} chunks.")

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunked_docs, embeddings)

print("Vector store created.")

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash-latest",
    temperature=0.2,
    convert_system_message_to_human=True
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)

chat_history = []

query_1 = "What is space tourism?"
response_1 = qa_chain.invoke({"question": query_1, "chat_history": chat_history})
print("Answer 1:", response_1["answer"])
chat_history.append((query_1, response_1["answer"]))

query_2 = "What are the ethical concerns?"
response_2 = qa_chain.invoke({"question": query_2, "chat_history": chat_history})
print("Answer 2:", response_2["answer"])

